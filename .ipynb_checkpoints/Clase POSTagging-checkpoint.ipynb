{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Clase 5: POS Tagging</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Qué es taguear? ¿Para qué nos sirve?\n",
    "\n",
    "● Asignarle a cada palabra su clase gramatical (‘part of speech’).  \n",
    "● Convertir un objeto sentence (lista de palabras) en una lista de tuplas\n",
    "(word, tag).  \n",
    "● Paso previo necesario antes casi cualquier otra tarea de más alto nivel:\n",
    "lemmatizing, chunking, parsing, word sense disambiguation.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esto vamos a utilizar un corpus, a fin de evaluar los taggers que vayamos desarrollando y, en algunos casos, también para entrenarlos.\n",
    "\n",
    "En principio podemos decir que hay dos clases de corpus:\n",
    "1. Corpus ‘crudos’: cualquier texto en formato digital, como los cuentos que usaron en clases anteriores.\n",
    "2. Corpus anotados: contienen algún tipo de anotación lingüística (clase de palabra, sintaxis, entidades, valoración, etc.)\n",
    "\n",
    "Para esta clase vamos a usar corpus con anotación de clase de palabra (part of speech, POS). En NLTK tenemos dos corpus con anotación de clase de palabra en español:\n",
    "\n",
    "● CoNLL2002 (Conference on Computational Natural Language Learning)  \n",
    "● CessEsp (corpus multilingüe de español y catalán)\n",
    "\n",
    "En este encuentro vamos a trabajar con el CoNLL2002.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El formato del CoNLL2002 es bastante simple, cada archivo del corpus tiene una palabra por línea, junto con su POStag y su namedEntity tag. Las líneas vacías señalan límites entre oraciones, y las líneas que dicen '-DOCSTART-' marcan límites entre artículos (el corpus se compone de artículos periodísticos).  \n",
    "\n",
    "Palabra POSTag namedEntityTag\n",
    "    \n",
    "Según SP O  \n",
    "los DA O  \n",
    "datos NC O  \n",
    "difundidos AQ O  \n",
    "hoy RG O  \n",
    "por SP O  \n",
    "Telefónica AQ B-ORG  \n",
    ", Fc O  \n",
    "la DA O  \n",
    "empresa NC O  \n",
    "ha VAI O  \n",
    "impuesto VMP O  \n",
    "en SP O  \n",
    "Sao NC B-LOC  \n",
    "Paulo VMI I-LOC  \n",
    "una DI O  \n",
    "marca NC O  \n",
    "mundial AQ O  \n",
    "en SP O  \n",
    "la DA O  \n",
    "expansión NC O  \n",
    "de SP O  \n",
    "redes NC O  \n",
    "de SP O  \n",
    "telefonía NC O  \n",
    "fija AQ O  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Cómo cargo un corpus en NLTK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import cess_esp\n",
    "from nltk.corpus import conll2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conll2002.readme())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_words = conll2002.words('esp.train')\n",
    "corpus_tagged_words = conll2002.tagged_words('esp.train')\n",
    "corpus_sents = conll2002.sents('esp.train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando cargamos un corpus utilizando nltk.corpus() disponemos de ciertos métodos propios de estos objetos:\n",
    "   \n",
    "● corpus.fileids()<br> \n",
    "● corpus.raw()  <br>\n",
    "● corpus.words()  <br>\n",
    "● corpus.sents()  <br>\n",
    "● corpus.tagged_sents()<br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(corpus_words))\n",
    "print(corpus_words[:10])\n",
    "print(corpus_sents[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos a inspeccionar un poco el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_info():\n",
    "    print('Palabras: ' + str(len(corpus_words)))\n",
    "    print('Oraciones: ' + str(len(corpus_sents)))\n",
    "    print('Lexico: ' + str(len(set(corpus_words))))\n",
    "\n",
    "corpus_info()\n",
    "\n",
    "def tagset_info():\n",
    "    tags = [str(tag) for (word, tag) in corpus_tagged_words]\n",
    "    tagset = set(tags)\n",
    "    print(\"Tagset\", tagset)\n",
    "    print(\"Cantidad de elementos en el tagset\", len(tagset))\n",
    "\n",
    "tagset_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frecuencia y frecuencia acumulada de las etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](tagset_dist.jpg \"Frecuencia acumulada de tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](tagsettable.jpg \"Frecuencia acumulada de tags\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taggers: modelos entrenables y no entrenables  \n",
    "\n",
    "A partir de acá, vamos a empezar a introducir, modelar y evaluar una serie de taggers. <br>\n",
    "\n",
    "Con 'taggers' nos referimos a algoritmos que toman como input un objeto sentence (una lista de palabras) y devuelven una lista de tuplas (palabra, tag). <br>\n",
    "\n",
    "Cualquiera de estos taggers no toma como input un texto 'crudo' (por ejemplo una string, o un archivo en formato .txt) sino un texto ya tokenizado.\n",
    "\n",
    "        -> Proceso: transformar una lista de palabras a una lista de tuplas (word, tag)  \n",
    "        -> Preproceso: tokenizar la oración \n",
    "        \n",
    "El módulo tokenize de nltk nos permite usar una función 'word_tokenize', que es un tokenizador genérico para cualquier idioma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = u'Arriba las manos! Llegó el estado.'\n",
    "dummy_tokenized = sentence.split(' ')\n",
    "print(dummy_tokenized)\n",
    "tokenized = word_tokenize(sentence)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que los corpus, todos los taggers generados en NLTK tienen algunos métodos en común:   \n",
    "        ● tag()  \n",
    "        ● tag_sents()  \n",
    "        ● untag()  \n",
    "        ● evaluate() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para trabajar con taggers entrenables es necesario dividir nuestros datos (en nuestro caso, el corpus con oraciones postaggeadas) en dos subconjuntos, uno de entrenamiento y otro de testeo. Lo que vamos hacer cuando trabajemos con taggers entrenables es generar un modelo a partir del conjunto de entrenamiento y evaluarlo posteriormente a partir del conjunto de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](traintest.png \"Train and test sets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged_sents = conll2002.tagged_sents('esp.train')\n",
    "test_tagged_sents = conll2002.tagged_sents('esp.testa')\n",
    "\n",
    "print(train_tagged_sents[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué sucedería si emplearamos los mismos datos para entrenar y para evaluar el modelo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DEFAULT TAGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import DefaultTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos cuál es el tag mas probable\n",
    "tags = [tag for (word, tag) in corpus_tagged_words]\n",
    "default_tag = nltk.FreqDist(tags).max()\n",
    "\n",
    "# Asignamos ese tag a cualquier token\n",
    "default_tagger = DefaultTagger(default_tag)\n",
    "\n",
    "print(default_tag)\n",
    "print(\"Test\", default_tagger.tag(['Hola', 'Mundo']))\n",
    "print(\"Default\", default_tagger.evaluate(test_tagged_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque parezca trivial e innecesario, lo que acabamos de hacer nos va a ser útil en dos sentidos:  \n",
    "\n",
    "● Establecer un baseline a partir del cual juzgar la performance del resto de los taggers  \n",
    "● Como vamos a ver más adelante, si otro tagger no puede asignarle ningún tag a cierto token, es posible utilizar este default_tagger para que asigne el tag más probable del corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. REGEX TAGGER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase RegexpTagger() toma como argumento una lista de tuplas, en la que el primer elemento de cada\n",
    "tupla es una expresión regular y el segundo elemento es el tag que se le va a asignar a las palabras que matchéen con ese pattern.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import RegexpTagger\n",
    "\n",
    "patterns = [\n",
    " (r'.*ing$', 'VBG'),                # gerunds\n",
    " (r'.*ed$', 'VBD'),                 # simple past\n",
    " (r'.*es$', 'VBZ'),                 # 3rd singular present\n",
    " (r'.*ould$', 'MD'),                # modals\n",
    " (r'.*\\'s$', 'NN$'),                # possessive nouns\n",
    " (r'.*s$', 'NNS'),                  # plural nouns\n",
    " (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers\n",
    " (r'.*', 'NC')                      # nouns (default)\n",
    " ]\n",
    "\n",
    "'''\n",
    "patterns = [\n",
    " (ur'.*ción$', 'NC')\n",
    "]\n",
    "'''\n",
    "\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regexp_tagger.tag(corpus_sents[3]))\n",
    "print(\"Regex tagger\", regexp_tagger.evaluate(test_tagged_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. AFFIX TAGGER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Affix tagger va a funcionar de manera similar al regexpTagger, solo que en vez de definir nosotros un conjunto de patterns, el tagger los va a aprender automáticamente basándose en afijos de una medida fija (uno, dos, tres, n caracteres), tanto al comienzo como al final de la palabra.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import AffixTagger\n",
    "\n",
    "tagger = AffixTagger(train_tagged_sents)\n",
    "print(\"Affix tagger\", tagger.evaluate(test_tagged_sents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:  \n",
    "● affix_length – The length of the affixes that should be considered during training and tagging. Use negative numbers\n",
    "for suffixes. (default = -3)  \n",
    "● min_stem_length – Any words whose length is less than min_stem_length+abs(affix_length) will be assigned a tag of\n",
    "None by this tagger. (default = 2)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_tagger = AffixTagger(train_tagged_sents, affix_length=-2)\n",
    "print(\"Affix tagger II\", suffix_tagger.evaluate(test_tagged_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. LOOKUP TAGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(corpus_words)\n",
    "cfd = nltk.ConditionalFreqDist(corpus_tagged_words)\n",
    "most_freq_words = [word for word, tag in fd.most_common(500)]\n",
    "print(most_freq_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almacena el tag más usual para las n palabras más usuales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likely_tags = dict((word, cfd[word].max()) for word in most_freq_words)\n",
    "lookup_tagger = nltk.UnigramTagger(model=likely_tags)\n",
    "print(\"Look up\", lookup_tagger.evaluate(test_tagged_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. BACKOFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backoff_tagger = nltk.UnigramTagger(model=likely_tags, backoff=regexp_tagger)\n",
    "\n",
    "#sent = corpus_sents[3]\n",
    "#sent = nltk.tokenize.word_tokenize(u'Nunca seré policía')\n",
    "sent = ['Nunca', 'seré', 'policía']\n",
    "print(backoff_tagger.tag(sent))\n",
    "print(\"Look up\", backoff_tagger.evaluate(test_tagged_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de los modelos\n",
    "\n",
    "### ¿Qué palabras tagueamos mal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bad_tagged_words(tagger):\n",
    "    bad_tagged = []\n",
    "    test_tags = [(word, tag) for sent in corpus_sents for (word, tag) in tagger.tag(sent)]\n",
    "    gold_tags = [(word, tag) for sent in train_tagged_sents for (word, tag) in sent]\n",
    "    for i, item in enumerate(test_tags):\n",
    "        if item!=gold_tags[i]:\n",
    "            # palabra, predicted tag, expected tag\n",
    "            bad_tagged.append((item[0], item[1], gold_tags[i][1]))\n",
    "    return bad_tagged\n",
    "\n",
    "bad = bad_tagged_words(backoff_tagger)\n",
    "print(bad[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué palabras tagueamos mal con más frecuencia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "bad_tagged_words = [i[0] for i in bad]\n",
    "counter = Counter(bad_tagged_words)\n",
    "sorted_counter = sorted(counter.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(sorted_counter[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sn\n",
    "#import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#import sklearn as skl\n",
    "\n",
    "selectedTags = [i for i,j in nltk.FreqDist(tags).most_common(15)]\n",
    "def confusion_matrix(tagger):\n",
    "    test_tags = [tag for sent in corpus_sents for (word, tag) in tagger.tag(sent)]\n",
    "    gold_tags = [tag for (word, tag) in corpus_tagged_words]\n",
    "    labels = set(gold_tags)\n",
    "    cm = skl.metrics.confusion_matrix(gold_tags, test_tags, \n",
    "                                     labels=selectedTags)\n",
    "    cm = cm / cm.astype(np.float).sum(axis=1)\n",
    "    return cm\n",
    "\n",
    "#cm = confusion_matrix(backoff_tagger)\n",
    "#df_cm = pd.DataFrame(cm, index = selectedTags, columns = selectedTags)\n",
    "#plt.figure(figsize = (7,7))\n",
    "#sn.heatmap(df_cm, annot=False, cmap=plt.cm.Blues)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "![alt text](heatmap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuál es el límite teórico de un look-up tagger, considerando un corpus que contenga todas las posibles ocurrencias de un\n",
    "lenguaje?  \n",
    "\n",
    "a. En el juego de la vida vos podés triunfar.  \n",
    "b. Juego al futbol todos los martes.  \n",
    "c. Como agua para chocolate.  \n",
    "d. Me como una pizza entera.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.text import Text\n",
    "my_text = Text(corpus_words)\n",
    "concordance = my_text.concordance('juego')\n",
    "print(concordance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance del look-up tagger con diferentes tamaños de diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(cfd, wordlist):\n",
    "    lt = dict((word, cfd[word].max()) for word in wordlist)\n",
    "    baseline_tagger = nltk.UnigramTagger(model=lt, backoff=nltk.DefaultTagger('NN'))\n",
    "    return backoff_tagger.evaluate(train_tagged_sents)\n",
    "\n",
    "def display():\n",
    "    import pylab\n",
    "    #Distribución de frecuencias de las palabras\n",
    "    freq_dist = nltk.FreqDist(corpus_words)\n",
    "    #Muestra la distribución de tags para cada palabra\n",
    "    cfd = nltk.ConditionalFreqDist(corpus_tagged_words)\n",
    "    sizes = 2 ** pylab.arange(16)\n",
    "    perfs = [performance(cfd, [x[0] for x in freq_dist.most_common(size)]) for size in sizes]\n",
    "    pylab.plot(sizes, perfs, '-bo')\n",
    "    pylab.title('Performance del Look-up tagger')\n",
    "    pylab.xlabel('Size')\n",
    "    pylab.ylabel('Performance')\n",
    "    pylab.show()\n",
    "\n",
    "#display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](lookup.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gramas\n",
    "\n",
    "Un n-grama es un subsecuencia de n items. En ese sentido, la clase de NLTK BigramTagger va a mirar dos items (la palabra que queremos taguear y la anterior), mientras que la clase TrigrammTagger va a decidir qué tag asignar teniendo en cuenta tres items. Estos dos taggers van a ser buenos lidiando con una de las características que tienen los lenguajes naturales, el hecho de que el postag de una palabra depende del contexto en que esa palabra se encuentre.\n",
    "\n",
    "Muchas palabras tienen diferentes postags de acuerdo a cómo sean usadas, como en el ejemplo que vimos más arriba de 'juego' que puede ser utilizada tanto como nombre común (su acepción más usual en nuestro corpus) y como verbo presente en primera persona del singular.\n",
    "\n",
    "La idea detrás de los taggers de n-gramas es que mirando las palabras previas a la palabra target podemos inferir con mayor precisión cuál es el tag adecuado que debemos asignar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. UNIGRAM TAGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "\n",
    "unitagger = UnigramTagger(train_tagged_sents)\n",
    "print(unitagger.tag(word_tokenize(u'El perro me mordió fiero')))\n",
    "print(\"Unigram\", unitagger.evaluate(test_tagged_sents))\n",
    "\n",
    "# WITH BACKOFF\n",
    "unitagger = UnigramTagger(train_tagged_sents, backoff=default_tagger)\n",
    "print(unitagger.tag(word_tokenize(u'El perro me mordió fiero')))\n",
    "print(\"Unigram con backoff\", unitagger.evaluate(test_tagged_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. BIGRAM & TRIGRAM TAGGERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import BigramTagger, TrigramTagger\n",
    "\n",
    "bitagger = BigramTagger(train_tagged_sents)\n",
    "print(\"Bigram\", bitagger.evaluate(test_tagged_sents))\n",
    "\n",
    "tritagger = TrigramTagger(train_tagged_sents)\n",
    "print(\"Trigram\", tritagger.evaluate(test_tagged_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse data problem  \n",
    "● ¿Por qué baja la accuracy?  \n",
    "● ¿Qué pasa cuando no puede taguear una palabra nueva?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = nltk.tokenize.word_tokenize(u'Fue a la guerra')\n",
    "sentence2 = nltk.tokenize.word_tokenize(u'Superman fue a la guerra')\n",
    "\n",
    "print(bitagger.tag(sentence))\n",
    "print(bitagger.tag(sentence2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MORE BACKOFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def more_backoff(train_tagged_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_tagged_sents, backoff=backoff)\n",
    "        return backoff\n",
    "\n",
    "backoff_tagger = more_backoff(train_tagged_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff=default_tagger)\n",
    "print(\"More backoff\", backoff_tagger.evaluate(test_tagged_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todos somos héroes anónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonimize_NP(corpus):\n",
    "    ANONYMOUS = \"anonymous\"\n",
    "    new = []\n",
    "    for s in corpus:\n",
    "        for i, (w, tag) in enumerate(s):\n",
    "            if tag == u\"NP\":  # NP = proper noun in Parole tagset.\n",
    "                s[i] = (ANONYMOUS, u\"NP\")\n",
    "        new.append(s)\n",
    "    return new\n",
    "\n",
    "anon_train_tagged_sents = anonimize_NP(train_tagged_sents)\n",
    "anon_test_tagged_sents = anonimize_NP(test_tagged_sents)\n",
    "\n",
    "anon_tagger = more_backoff(anon_train_tagged_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff=default_tagger)\n",
    "print(\"Anon tagger\", anon_tagger.evaluate(anon_test_tagged_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escritura automática a partir de bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from random import choice\n",
    "import os\n",
    "\n",
    "\n",
    "def max_model(cfdist, word, num=15):\n",
    "    autotext = \"\"\n",
    "    for i in range(num):\n",
    "        autotext += word + \" \"\n",
    "        word = cfdist[word].max()\n",
    "    return autotext\n",
    "\n",
    "\n",
    "def choice_model(cfdist, word, num=15):\n",
    "    autotext = \"\"\n",
    "    for i in range(num):\n",
    "        autotext += word + \" \"\n",
    "        wordlist = list(cfdist[word])\n",
    "        #print(wordlist)\n",
    "        word = choice(wordlist)\n",
    "    return autotext\n",
    "\n",
    "\n",
    "def poem(text, cfd, model, lines=10, num=10):\n",
    "    poem = ''\n",
    "    for i in range(lines):\n",
    "        word = choice(text)\n",
    "        autotext = model(cfd, word, num)\n",
    "        poem += autotext + '\\n'\n",
    "    return poem\n",
    "\n",
    "corpus = PlaintextCorpusReader(os.getcwd(), \"[a-zA-Z0-9]*.txt\")\n",
    "text = corpus.words('martinfierro.txt')\n",
    "bigrams = nltk.bigrams(text)\n",
    "cfd = nltk.ConditionalFreqDist(bigrams)\n",
    "autotext1 = max_model(cfd, 'Cuando')\n",
    "autotext2 = choice_model(cfd,\"Cuando\")\n",
    "\n",
    "print(autotext1)\n",
    "print('\\n')\n",
    "print(poem(text, cfd, max_model))\n",
    "print('\\n')\n",
    "print(poem(text, cfd, choice_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rick.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "Pickles es un módulo de python que nos permite serializar objetos, es decir, guardar en un archivo cualquier objeto que hayamos instanciado en python.\n",
    "\n",
    "Los taggers que vienen a continuación requieren un entrenamiento, por lo que vamos a tardar bastante tiempo en generar el tagger. Por eso, una vez entrenados resulta conveniente almacenarlos en un archivo usando pickle, para poder reutilizarlos posteriormente sin necesidad de re-entrenarlos.\n",
    "\n",
    "Tiene dos métodos básicos: dump() para guardar un objeto en un archivo y load() para cargarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "dummyVariable = \"Nunca seré policía, de provincia ni de capital\"\n",
    "# Serializo el objeto dummyVariable\n",
    "pickle.dump(dummyVariable, open(\"fuck.p\", \"wb\"))\n",
    "\n",
    "# Lo cargo en una nueva variable\n",
    "loadVariable = pickle.load(open(\"fuck.p\", \"rb\"))\n",
    "print(loadVariable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. BRILL TAGGER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Brill tagger es un método inductivo para taggear. Fue desarrollado en 1993 por Eric Brill en su tesis de doctorado (https://dl.acm.org/citation.cfm?doid=974499.974526). <br>\n",
    "\n",
    "Puede ser descripto como un tagger transformacional basado en minimizar la cantidad de errores. Vendría a ser una forma de aprendizaje supervisado que intenta minimizar el error a partir de un proceso transformacional, en el sentido de que cada tag es asignado a cada palabra y luego es revisado (y potencialmente cambiado) a partir de un set de reglas predefinidas que fueron inferidas del corpus. <br>\n",
    "\n",
    "Aplicando iterativamente estas reglas, modificando los tags incorrectos, logra aumentar la precisión de cualquiera de los tags que vimos anteriormente. Las reglas que genera automáticamente permiten inferir información valiosa, como son las reglas morfosintácticas de combinación de las palabras, que luego son utilizadas en el proceso de tagueo.\n",
    "\n",
    "Es decir que, una vez que a cada palabra se le ha asignado un tag provisional, una serie de reglas contextuales son aplicadas iterativamente, con el objetivo de \"corregir\" tags erroneos, a partir del exámen de pequeñas cantidades de contexto.\n",
    "\n",
    "tag1 → tag2 IF Condition\n",
    "1. Utiliza otro tagger para asignarle tags provisorios a cada palabra.  \n",
    "2. Aplica iterativamente reglas basadas en el contexto.  \n",
    "3. A cada regla se le asigna un puntaje (errores que corrige - errores que produce).  \n",
    "4. Se queda con las reglas que maximizan la performance del tagger.\n",
    "5. Las aplica a todo el análisis.  \n",
    "\n",
    "Utiliza dos argumentos:  \n",
    "● Tagger  \n",
    "● Lista de templates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import brill, brill_trainer\n",
    "\n",
    "def train_brill_tagger(initial_tagger, train_sents, **kwargs):\n",
    "    templates = [\n",
    "        brill.Template(brill.Pos([-1])),\n",
    "        brill.Template(brill.Pos([1])),\n",
    "        brill.Template(brill.Pos([-2])),\n",
    "        brill.Template(brill.Pos([2])),\n",
    "        brill.Template(brill.Pos([-2, -1])),\n",
    "        brill.Template(brill.Pos([1, 2])),\n",
    "        brill.Template(brill.Pos([-3, -2, -1])),\n",
    "        brill.Template(brill.Pos([1, 2, 3])),\n",
    "        brill.Template(brill.Pos([-1]), brill.Pos([1])),\n",
    "        brill.Template(brill.Word([-1])),\n",
    "        brill.Template(brill.Word([1])),\n",
    "        brill.Template(brill.Word([-2])),\n",
    "        brill.Template(brill.Word([2])),\n",
    "        brill.Template(brill.Word([-2, -1])),\n",
    "        brill.Template(brill.Word([1, 2])),\n",
    "        brill.Template(brill.Word([-3, -2, -1])),\n",
    "        brill.Template(brill.Word([1, 2, 3])),\n",
    "        brill.Template(brill.Word([-1]), brill.Word([1])),\n",
    "    ]\n",
    "    trainer = brill_trainer.BrillTaggerTrainer(initial_tagger, templates, deterministic=True, trace=True)\n",
    "\n",
    "    return trainer.train(train_sents, max_rules=20, **kwargs)\n",
    "\n",
    "initial_tagger = more_backoff(train_tagged_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff=default_tagger)\n",
    "#BrillTagger = train_brill_tagger(initial_tagger, train_tagged_sents)\n",
    "#pickle.dump(BrillTagger, open(\"BrillTagger.p\", \"wb\"))\n",
    "BrillTagger = pickle.load(open(\"BrillTagger.p\", \"rb\"))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "for rule in BrillTagger.rules(): print(str(rule.format('verbose')).encode('utf-8'))\n",
    "print(\"\\n\\n\")\n",
    "print(BrillTagger.train_stats())\n",
    "print(\"\\n\\n\")\n",
    "print(\"Brill tagger\", BrillTagger.evaluate(test_tagged_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taggers basados en clasificadores automáticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El postagging puede ser entendido como un problema de clasificación, en el que debemos clasificar ciertos inputs (tokens) entre una lista de categorías posibles (tags). Para automatizar esta clasificación podemos valernos de modelos estadísticos. Estos van a predecir las probabilidades de cada tag para cada uno de los inputs a partir de un conjunto de variables independientes que nosotros definamos.\n",
    "\n",
    "2 componentes:  \n",
    "● Feature extractor  \n",
    "● Machine learning algorithm  \n",
    "\n",
    "Clasificación supervisada:  \n",
    "● Asigna una etiqueta para determinado input  \n",
    "● Basándose en un corpora de entrenamiento que contiene las etiquetas correctas. \n",
    "\n",
    "![alt text](1.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este es el source code de NLTK, vamos a intentar leer la función de feature_detector que utiliza:\n",
    "\n",
    "from nltk.tag.sequential import ClassifierBasedTagger\n",
    "\n",
    "class ClassifierBasedPOSTagger(ClassifierBasedTagger):\n",
    "    \"\"\"\n",
    "    A classifier based part of speech tagger.\n",
    "    \"\"\"\n",
    "\n",
    "    def feature_detector(self, tokens, index, history):\n",
    "        word = tokens[index]\n",
    "        if index == 0:\n",
    "            prevword = prevprevword = None\n",
    "            prevtag = prevprevtag = None\n",
    "        elif index == 1:\n",
    "            prevword = tokens[index-1].lower()\n",
    "            prevprevword = None\n",
    "            prevtag = history[index-1]\n",
    "            prevprevtag = None\n",
    "        else:\n",
    "            prevword = tokens[index-1].lower()\n",
    "            prevprevword = tokens[index-2].lower()\n",
    "            prevtag = history[index-1]\n",
    "            prevprevtag = history[index-2]\n",
    "\n",
    "        if re.match('[0-9]+(\\.[0-9]*)?|[0-9]*\\.[0-9]+$', word):\n",
    "            shape = 'number'\n",
    "        elif re.match('\\W+$', word):\n",
    "            shape = 'punct'\n",
    "        elif re.match('[A-Z][a-z]+$', word):\n",
    "            shape = 'upcase'\n",
    "        elif re.match('[a-z]+$', word):\n",
    "            shape = 'downcase'\n",
    "        elif re.match('\\w+$', word):\n",
    "            shape = 'mixedcase'\n",
    "        else:\n",
    "            shape = 'other'\n",
    "\n",
    "        features = {\n",
    "            'prevtag': prevtag,\n",
    "            'prevprevtag': prevprevtag,\n",
    "            'word': word,\n",
    "            'word.lower': word.lower(),\n",
    "            'suffix3': word.lower()[-3:],\n",
    "            'suffix2': word.lower()[-2:],\n",
    "            'suffix1': word.lower()[-1:],\n",
    "            'prevprevword': prevprevword,\n",
    "            'prevword': prevword,\n",
    "            'prevtag+word': '%s+%s' % (prevtag, word.lower()),\n",
    "            'prevprevtag+word': '%s+%s' % (prevprevtag, word.lower()),\n",
    "            'prevword+word': '%s+%s' % (prevword, word.lower()),\n",
    "            'shape': shape,\n",
    "            }\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Naive Bayes Classiffier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este clasificador 'ingenuo' se basa en el teorema de Bayes:\n",
    "\n",
    "![alt text](bayes.jpeg)\n",
    "\n",
    "1. Calcula la probabilidad previa de cada tag, chequeando la etiqueta de cada uno en el corpus de entrenamiento.  \n",
    "2. Cada rasgo contribuye “votando en contra” de aquellas etiquetas que no co-ocurren con él.  \n",
    "\n",
    "<center>P(features, label) = P(label) × P(features|label)</center>\n",
    "\n",
    "![alt text](5.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "\n",
    "#NaiveBayesTagger = ClassifierBasedPOSTagger(train=train_tagged_sents)\n",
    "#pickle.dump(NaiveBayesTagger, open(\"NaiveBayesTagger.p\", \"wb\"))\n",
    "NaiveBayesTagger = pickle.load(open(\"NaiveBayesTagger.p\", \"rb\"))\n",
    "print(\"Bayes Classifier\", NaiveBayesTagger.evaluate(test_tagged_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión:  \n",
    "\n",
    "●Los taggers basados en clasificadores automáticos son los que tienen mejor performance, pero también son los más lentos.  \n",
    "●Si necesitamos un modulo rápido, lo mejor es usar un Bril Tagger precedido por una cadena de taggers basados en n-gramas.  \n",
    "\n",
    "--> Brill tagger es un modelo más robusto y económico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El desarrollo de taggers tuvo un rol central en el crecimiento de los enfoques estadísticos en el procesamiento del lenguaje natural.  \n",
    "En los comienzos de la década de los 90' la performance de los taggers estadísticos brindó una prueba certera de que se podía resolver al menos una pequeña porción del entendimiento linguistico sin necesidad de acceder a un conocimiento más profundo de la estructura gramatical como es la sintaxis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lematización\n",
    "\n",
    "Lematizar es asignarle a cada palabra su forma de diccionario. Al igual que sucede con los algoritmos de stemming, es un proceso sumamente util para reducir la dimensionalidad de los datos y así mejorar tanto la eficiencia y como la eficacia de cualquier algoritmo de más alto nivel (un clasificador, por ejemplo).\n",
    "\n",
    "Una vez que tenemos la clase de palabra de cada forma léxica, si contamos con un diccionario de formas y lemmas posibles (como el que nos provee freeling ;) ) lematizar se convierte en algo bastante trivial.\n",
    "\n",
    "En el archivo dicc.src tenemos el diccionario de Freeling. Está organizado de esta manera:\n",
    "\n",
    "promedio promediar VMIP1S0 promedio NCMS000 <br>\n",
    "que que CS que PR0CN000 <br>\n",
    "no no NCMS000 no RN <br>\n",
    "noble noble AQ0CS0 noble NCCS000 <br>\n",
    "\n",
    "El primer ítem es una forma léxica. A partir de ahí puede haber cualquier cantidad de pares lemma/tag, tantos como posibles tags tenga la palabra.\n",
    "\n",
    "[Acá](/edit/lematizador.py) está el código de un lematizador bastante trivial que armé. Lo que hace es taguear el texto que querramos lematizar y luego buscar en el diccionario de freeling a qué lemma de esa forma léxica le corresponde ese tag. Para que la búsqueda sea más rápida, convertimos el archivo de texto en un diccionario de python y lo guardé en el archivo freeling_dicc.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lematizador import Lemmatizer\n",
    "\n",
    "myLemmatizer = Lemmatizer(NaiveBayesTagger, debug=True)\n",
    "myLemmatizer.build_dicc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = u\"Nunca te creí. Las calles de Buenos Aires tienen esas casas azules.\"\n",
    "myLemmatizer.process(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
